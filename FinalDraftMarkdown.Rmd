---
title: "Ocean Conservancy, Urban Ocean Marine Debris Prediction"
subtitle: "Trevor Kapuvari, Shreya Bansal, Tianxiao Chen, Stephanie Cheng, Xiaofan Liu"
author: "University of Pennsylvania"
date: "2024-02-22"
output: 
  html_document:
    css: style.css
    toc: true
    toc_float: true
    code_folding: "hide"
    theme: flatly
    highlight: tango
    number_sections: yes
mainfont: DejaVu Sans
editor_options: 
  markdown: 
    wrap: 72
---


# 1. Introduction

Pollution is a global problem that takes many forms contaminating every aspect of the environment. Some of this pollution, specifically plastic-waste, ends up in our oceans, harming marine life, contaminating drinking water, and hurting local economies. In response, Ocean Conservancy's Urban Ocean program has been developing projects that mitigate marine pollution, assess waste management, and enable cities to address ocean plastics and resilience. These projects intend to deploy "zero-waste" pilot strategies in cities around the world. With this wide-spanning outreach and diversity in partnerships, an assessment on site selection and resource allocation becomes a repeated step in the process that hinders efficiency each waste-reduction campaign. 

Our objective is to develop a site assessment model that identifies effective zero-waste site locations for national and multinational use. This geospatial risk assessment model serves the purpose of predicting litter accumulation based on globally sourced data with a repeatable framework on an international scale. The results of the model aim to evaluate which sections of a given area have the highest likelihood to produce and contain litter relative to its surroundings. For our case, the Urban Ocean program intends to dedicate "zero-waste" solutions in these areas for the most effective impact for each deployment.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages("remotes")
#remotes::install_github("IREA-CNR-MI/sprawl")

library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)
library(RCurl)
library(httr)
library(osmdata)
library(randomForest)
library(tidygraph)
library(XML)
library(neuralnet)
library(MASS)
library(tidymodels)
library(brms)
library(jsonlite)
library(QuickJSR)
library(hash)
library(fastDummies)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
source('https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/function_UO.R')
st_c    <- st_coordinates
st_coid <- st_centroid

#lists!
cities <- c("Bangkok", "Can_Tho", "Chennai", "Melaka", "Mumbai", "Panama_City", 
            "Pune", "Salvador", "Santa_Fe", "Santiago", "Semarang", "Surat")
bd <- c("Bangkok_bd", "Can_Tho_bd", "Chennai_bd", "Melaka_bd", "Mumbai_bd", "Panama_City_bd", "Pune_bd", "Salvador_bd", "Santa_Fe_bd", "Santiago_bd", "Semarang_bd", "Surat_bd")
bdM <- c("Bangkok_bdM", "Can_Tho_bdM", "Chennai_bdM", "Melaka_bdM", "Mumbai_bdM", "Panama_City_bdM","Pune_bdM", "Salvador_bdM", "Santa_Fe_bdM", "Santiago_bdM", "Semarang_bdM", "Surat_bdM")

# pre-store of osm data need to use 
# a more expandable version of function
# the actual store order is cate label, small cate
stor_df <- data.frame(cato = character(), small =list(), label = character(),stringsAsFactors = FALSE)
add_row <-function(cato,small,label){
  new_row <- list(cato = cato, small = small, label = label)
  stor_df <- bind_rows(stor_df, new_row)
  return(stor_df)
}
stor_df <- add_row('water',list(c('canal','drain','ditch')), 'water')
stor_df <- add_row('amenity',list(c('waste_basket','waste_disposal','waste_transfer_station','recycling')), 'waste')
stor_df <- add_row('amenity',list(c('restaurant','pub','bar')), 'restaurant')
stor_df <- add_row('highway',list('residential'), 'road')
stor_df <- add_row('landuse',list('industrial'), 'industrial')
stor_df <- add_row('landuse',list('residential'), 'residential')
stor_df <- add_row('landuse',list('retail'), 'retail')
```

# Initial Analysis and Preperation

For our model, we directed our focus on various cities based from the Resilient Cities Network. These cities displayed are Chennai India; Bangkok, Thailand; and Santiago, Chile. The three cities mentioned are apart of a 12 city networking with the goal of proactively enhancing their sanitation infrastructure to combat plastic pollution in their region. The data regarding each region and its attributes was sourced from OpenStreetMap (OSM) to ensure a repeatable framework & quality consistency. 

To develop our model for each respective city (or future use), each area was determined by a 'boundary box' that was created by a custom KML file using Google My Maps (<https://www.google.com/mymaps>). Each boundary's drawing was dictated by political boundaries and practical scope of where data was recorded within the city, further methodology of each individual city is detailed on the interactive dashboard. 
 
## Data Sources and Methodology

In terms of our data & variables, acquiring litter data, our primary environmental health indicator, was acquired via manual download data from Marine Debris Tracker (<https://www.debristracker.org/data/>). The data for each city was carved out by a boundary box determined by initial-research political boundaries. From there, all debris recorded between January 2021 - February 2024 was compiled to a CSV and used as our primary independent variable. Each selected city required its own evaluation in terms of usable debris data over a several year time span. Ideal locations featured several thousand data points across 2-3 years for early & late stage model development. Despite the scalability of the model itself, results and accuracy lay contingent on data quantity and external factors available to each region. 

After the boundary box was created, the data was then projected to a fishnet grid. The fishnet grid was used to count the number of litter points in each cell in a format that can be compared to by each grid cell in the city.  

```{r}

```


```{r chennai data, warning = FALSE, message = FALSE, results ='hide'}
#step to load city's data
litter <- read.csv('https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/Data/mdt-dataChennai.csv')

# data filter and projection transformation
litter_p <- litter%>%filter(master_material == 'PLASTIC')%>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant")%>%st_transform('EPSG:32643')

#img <- raster("/Users/mr.smile/Desktop/UPENN/Spring24/CPLN790/data/population_ind_pak_general/population_10_lon_80_general-v1.5.tif")

chen_bdry <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Chennai.kml')
chen_bdry <- st_set_crs(chen_bdry, 4326)%>%st_transform('EPSG:32643')
temp_bd <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Chennai.kml')

temp_bbox <- get_bbox(temp_bd) # get the bounding box (the projection of temp_bd should be epsg4326)
temp_fish <- create_fish(chen_bdry) # get the fishnet of the city (the projection of chen_bdry should be meter degree)
final_net <- countfishnet(temp_fish, litter_p) # create base fishnet with litter (also the one used as final one)
final_net <- pn_gen(stor_df) # add osm point data and knn calculation result into the final dataset
#temp_point <- raster_process(img,temp_bd) # convert the raster file to point one 
#pop_result <- pop_process(temp_point, temp_fish, 32643) # summary the population result
#final_net <- add_pop(pop_result,final_net) # add the pop result into the final dataset
final_net <- moran_gen(final_net,stor_df) # calculate the moran's I result into the dataset
# DONE! 
chen_net <- final_net
```

```{r bangkok data, warning = FALSE, message = FALSE}
litter <- read.csv('https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/Data/mdt-dataBangkok.csv')

litter_b <- litter%>%
  filter(master_material == 'PLASTIC')%>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant")%>%
  st_transform('EPSG:32643')
litter_b <- subset(litter_b, select = -c(event_name, project_name))

bok_bdry <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Bangkok.kml')
bok_bdry <- st_set_crs(bok_bdry, 4326)%>%
  st_transform('EPSG:32643')

bok_bd <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Bangkok.kml')

temp_bbox <- get_bbox(bok_bd) 
temp_fish <- create_fish(bok_bdry)
final_net <- countfishnet(temp_fish, litter_b) 
final_net <- pn_gen(stor_df) 
final_net <- moran_gen(final_net,stor_df) 
bok_net <- final_net
```

```{r santiago data, warning = FALSE, message = FALSE}
litter <- read.csv('https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/Data/mdt-dataSantiago.csv')

# data filter and projection transformation
litter_s <- litter%>%
  filter(master_material == 'PLASTIC')%>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant")%>%
  st_transform('EPSG:32643')
litter_s <- subset(litter_s, select = -c(event_name, project_name))

# load boundary data

san_bdry <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Santiago.kml')
san_bd <- san_bdry
san_bdry <- st_set_crs(san_bdry, 4326)%>%
  st_transform('EPSG:32643')

temp_bbox <- get_bbox(san_bd) 
temp_fish <- create_fish(san_bdry)
final_net <- countfishnet(temp_fish, litter_s) 
final_net <- pn_gen(stor_df) 
final_net <- moran_gen(final_net,stor_df)
san_net <- final_net
```

```{r aggregate data}
chen_net <- chen_net %>%
 # dplyr::select(!c(avg_pop ,sum_pop))%>%
  mutate(city = 'Chennai',
         country = 'India')

san_net <- san_net %>%
  mutate(city = 'Santiago',
         country = 'Chile')

bok_net <- bok_net %>%
  mutate(city = 'Bangkok',
         country = 'Thailand')

tt_net <- rbind(chen_net,san_net,bok_net) %>% mutate(uniqueID = 1:n())

# reference: https://wiki.openstreetmap.org/wiki/Map_features#Entertainment,_Arts_&_Culturee 
leisure <- c('park')
act <- c('maxspeed') 
```


The litter data features a wide variety of item categories and general information about each piece. Despite the quality of detail for each recorded sample, this only accounts for litter that has been identified, recorded, and disposed of. This data does not account for litter that was identified but never disposed of, accounted for, or assumed. Regardless, areas where litter was not recorded in a general surrounding could not confidently be assumed to be present or not present.

## 2.2 Marine Debris Data, in Summary

Each chart below visualizes a dependent variable reformatted for repeatable use  an in a statistical context that better represents its relation to litter. The examples are, restaurants, roads, retail proximity, and significant presence of restaurants. Our variable selection was based on our hypothesis of areas of human activity leading to higher litter risk. The aim of these variables is to act as proxies for litter cases, providing insight on where litter most likely accumulated or ends up. These variables, similarly to litter, are analyzed on a fishnet grid where the quantity is counted per cell. Each variable is then placed on a combined fishnet grid with the litter data, computed using a Chi-Squared test, and evaluated for association between the variables and litter.

Restaurants are used as a point of significance because of their relation to commercial activity. We wanted to examine a 'significance' analysis because of their direct implication with high human activity, excess flow of goods, and interconnection of urban systems. Areas of low distances of a "nearest neighbor" imply restaurants are close to one another geographically, indicate density in resources, and identify an urban core. 


```{r visualize count& continuous}
#visual_count(chen_net,'waste')
#visual_count(chen_net,'water')
visual_count(chen_net,"restaurant")
visual_count(chen_net,'road')
#visual_count(bok_net,'industrial')
#visual_count(chen_net,"residential")
#visual_count(chen_net,'retail')

#visual_cotinuous(chen_net,'waste_nn')
#visual_cotinuous(san_net,'water_nn')
#visual_cotinuous(san_net,"restaurant_nn")
#visual_cotinuous(san_net,'road_nn')
#visual_cotinuous(san_net,'industrial_nn')
#visual_cotinuous(san_net,"residential_nn")
visual_cotinuous(san_net,'retail_nn')

#visual_count(bok_net,'waste_sig')
#visual_count(bok_net,'water_sig')
visual_count(bok_net,"restaurant_sig")
#visual_count(bok_net,'road_sig')
#visual_count(bok_net,'industrial_sig')
#visual_count(bok_net,"residential_sig")
#visual_count(bok_net,'retail_sig')

#visual_cotinuous(bok_net,'waste_sig_dis')
#visual_cotinuous(bok_net,'water_sig_dis')
#visual_cotinuous(bok_net,"restaurant_sig_dis")
#visual_cotinuous(bok_net,'road_sig_dis')
#visual_cotinuous(bok_net,'industrial_sig_dis')
#visual_cotinuous(bok_net,"residential_sig_dis")
#visual_cotinuous(bok_net,'retail_sig_dis')

```

## 2.3 Correlation Analysis 

The following correlation analysis shows the various features of our model. This matrix allows for further PCA analysis and variable selection.

```{r correlation matrix}
cor_chen <- st_drop_geometry(chen_net) %>% dplyr::select(!c(uniqueID,cvID,city,country))
cor_nor_chen<- scale(cor_chen)
corr_matrix <- cor(cor_nor_chen)
ggcorrplot(corr_matrix)
```

## 2.4 Principal Component Analysis 

The PCA analysis below is used to identify the most important variables in the dataset and their correlation with litter. The reason why we perform PCA is to decrease the dimension of variable. The following result can get explained by the color and direction in the last map. and the last PCA map is the basic for choosing the several variables. Based on the result, the selected variables included 'waste_sig_dis, restaurant_sig_dis, residential_sig_dis, water_sig_dis, residential_nn, industrial_sig_dis, industrial_sig, restaurant_sig, industrial_nn, road_sig_dis, residential_sig, restaurant_sig, restaurant' as the 'shortened model' independent variables.

```{r PCA analysis}
data.pca <- princomp(corr_matrix)
summary(data.pca)
data.pca$loadings[, 1:2]

fviz_eig(data.pca, addlabels = TRUE)

fviz_pca_var(data.pca, col.var = "black")

fviz_cos2(data.pca, choice = "var", axes = 1:2)

fviz_pca_var(data.pca, col.var = "cos2",
            gradient.cols = c("black", "orange", "green"),
            repel = TRUE)
```

# 3. Model Building

## 3.1 Reduced Bias Model

In light of the inherent bias stemmed from litter data, we also are developing a 'Reduced Bias' Model in attempt to compensate for potential  assumptions made by the model in terms of quantity. We take each indicator and coalesce them into a single variable used for our model to compare litter data to the indicators.

```{r model build}
# prepare for model data
df_model <- st_drop_geometry(tt_net)%>%dplyr::select(!cvID)
df_model <- dummy_cols(df_model, select_columns = "city")
df_model <- dummy_cols(df_model, select_columns = "country")
df_model <- df_model %>% dplyr::select(!c(city,country)) %>% mutate(uniqueID = 1:n())

# several model 
temp.rf <- randomForest(count ~ ., data = df_model%>%dplyr::select(!uniqueID), mtry = 10,ntree=70, 
                         importance = TRUE, na.action = na.omit) 

temp.lr <- glm(count ~ ., data = df_model%>%dplyr::select(!uniqueID),family = "poisson", na.action = na.omit) 

temp.lr.qs <- glm(count ~ ., data = df_model%>%dplyr::select(!uniqueID),family = "quasi", na.action = na.omit) 

temp.lr.qp <- glm(count ~ ., data = df_model%>%dplyr::select(!uniqueID),family = "quasipoisson", na.action = na.omit) 

nn_model = neuralnet(count ~ .,data=df_model%>%dplyr::select(!uniqueID),hidden=c(5,2),
linear.output = TRUE
)

temp.hbr <- fit.1<- brm(count~ ., data=df_model%>%dplyr::select(!uniqueID), family=gaussian(),warmup=500, iter=1000, chains=2,cores=2,seed = 1115)
```

## 3.2 Model Results

The following model results of each of the models was added to the dataframe. We test a range of statistical models for a comparative analysis, including random forest, linear model, quasi poisson, hierarchical bayes regression. This section also includes a decision tree model. While this model has a strong visualization of the risk area, its accuracy was not strong.

```{r decision tree model}
set.seed(123)
data_split <- initial_split(df_model, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)

# Create a decision tree model specification
tree_spec <- decision_tree(mode = "regression", tree_depth = 4,engine = 'rpart') 

# Fit the model to the training data
tree_fit <- tree_spec %>%
 fit(count ~ ., data = train_data)

predictions <- tree_fit %>%
 predict(test_data) %>%
 pull(.pred)

metrics <- metric_set(rmse, rsq)
model_performance <- test_data %>%
 mutate(predictions = predictions) %>%
 metrics(truth = count, estimate = predictions)

print(model_performance)

predictions <- tree_fit %>%
 predict(df_model)%>%
 pull(.pred)

df_dt_rst <- df_model%>%
  mutate(Prediction = predictions)%>%
  dplyr::select(uniqueID,count,Prediction)

df_dt_rst <- left_join(final_net%>%dplyr::select(uniqueID),df_dt_rst, by="uniqueID")

#risk_v(df_dt_rst,litter_p,"kmeans")
```

```{r model result}
df_rf_rst <- model_process(df_model,temp.rf)
df_lr_rst <- model_process(df_model,temp.lr)
df_lrqs_rst <- model_process(df_model,temp.lr.qs)
df_lrqp_rst <- model_process(df_model,temp.lr.qp)
df_nn_rst <- model_process(df_model,nn_model)

df_brm_rst <- model_process(df_model,temp.hbr)
estimates_list <- list()
# Iterate over each row by index
for(i in 1:nrow(df_brm_rst)) {
  # Extract the 'Prediction' for the current row
  current_prediction <- df_brm_rst$Prediction[[i]][1]
  estimates_list[[i]] <- current_prediction
}

df_brm_rst$Estimate <- estimates_list
df_brm_rst <- df_brm_rst %>%
  dplyr::select(uniqueID,count,Estimate)%>%
  rename(Prediction = Estimate)

df_brm_rst$Prediction <- as.numeric(unlist(df_brm_rst$Prediction))
#df_brm_r <- model_result(df_model,fit.1) %>%mutate(model = 'BRM')


#df_rf_r <- model_result(df_model,temp.rf) %>%mutate(model = 'RF')
#df_lr_r <- model_result(df_model,temp.lr) %>%mutate(model = 'LR')
#df_lrqs_r <- model_result(df_model,temp.lr.qs)%>%mutate(model = 'LR_quasi')
#df_lrqp_r <- model_result(df_model,temp.lr.qp)%>%mutate(model = 'LR_quasipoisson')
#df_nn_r <- model_result(df_model,nn_model)%>%mutate(model = 'Neural Net')
#df_r_tt <- do.call("rbind", list(df_rf_r, df_lr_r, df_lrqs_r,df_lrqp_r,df_nn_r))

df_rf_rst <- left_join(tt_net%>%dplyr::select(uniqueID,city),df_rf_rst, by="uniqueID")
df_lr_rst <- left_join(tt_net%>%dplyr::select(uniqueID,city),df_lr_rst, by="uniqueID")
df_lrqs_rst <- left_join(tt_net%>%dplyr::select(uniqueID,city),df_lrqs_rst, by="uniqueID")
df_lrqp_rst <- left_join(tt_net%>%dplyr::select(uniqueID,city),df_lrqp_rst, by="uniqueID")
df_brm_rst <- left_join(tt_net%>%dplyr::select(uniqueID,city),df_brm_rst, by="uniqueID")

```

## 3.3 Model Testing Across Different Cities

For our various models, we have tested them on Bangkok as well. The results are displayed below. Each model has a different level of sensitivity.

Below shows the various models for Chennai and their predictions of litter risk across 5 categories of intensity.


```{r model test}


df_rf_c <- df_rf_rst %>% filter(city == 'Chennai')
df_lr_c <- df_lr_rst %>% filter(city == 'Chennai')
df_lrqs_c <- df_lrqs_rst %>% filter(city == 'Chennai')
df_lrqp_c <- df_lrqp_rst %>% filter(city == 'Chennai')
df_brm_c <- df_brm_rst %>% filter(city == 'Chennai')

grid.arrange(
  risk_v(df_rf_c,litter_p,"kmeans",'random forest'),
  risk_v(df_lr_c,litter_p,"kmeans",'linear regression'),
  risk_v(df_lrqs_c,litter_p,"kmeans",'Lr-quasi'),
  risk_v(df_lrqp_c,litter_p,"kmeans",'Lr-quasipossion'), 
  risk_v(df_brm_c,litter_p,"kmeans",'HrB regression'),nrow = 2
)
```

Below shows the various models for Bangkok and their predictions of litter risk across 5 categories of intensity.

```{r bangkok risk_normal}
df_rf_b <- df_rf_rst %>% filter(city == 'Bangkok')
df_lr_b <- df_lr_rst %>% filter(city == 'Bangkok')
df_lrqs_b <- df_lrqs_rst %>% filter(city == 'Bangkok')
df_lrqp_b <- df_lrqp_rst %>% filter(city == 'Bangkok')
df_brm_b <- df_brm_rst %>% filter(city == 'Bangkok')
grid.arrange(
  risk_v(df_rf_b,litter_b,"kmeans",'random forest'),
  risk_v(df_lr_b,litter_b,"kmeans",'linear regression'),
  risk_v(df_lrqs_b,litter_b,"kmeans",'Lr-quasi'),
  risk_v(df_lrqp_b,litter_b,"kmeans",'Lr-quasipossion'),
  risk_v(df_brm_b,litter_b,"kmeans",'HrB regression'),nrow = 2
)
```

The following comparative model visaulizes the strength of the model as it is used acrossed cities.

```{r normal_compare}
grid.arrange(
#risk_v(df_rf_s,litter_s,'kmeans'),
risk_v(df_rf_c,litter_p,'kmeans','random forest'),
risk_v(df_rf_b,litter_b,'kmeans','random forest'),nrow = 1)

grid.arrange(
#risk_v(df_lr_s,litter_s,'kmeans'),
risk_v(df_lr_c,litter_p,'kmeans','linear regression'),
risk_v(df_lr_b,litter_b,'kmeans','linear regression'),nrow = 1)

grid.arrange(
#risk_v(df_lrqs_s,litter_s,'kmeans'),
risk_v(df_lrqs_c,litter_p,'kmeans','Lr-quasi'),
risk_v(df_lrqs_b,litter_b,'kmeans','Lr-quasi'),nrow = 1)

grid.arrange(
#risk_v(df_lrqp_s,litter_s,'kmeans'),
risk_v(df_lrqp_c,litter_p,'kmeans','Lr-quasipossion'),
risk_v(df_lrqp_b,litter_b,'kmeans','Lr-quasipossion'),nrow = 1)
```
