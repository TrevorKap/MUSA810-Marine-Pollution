---
title: "Ocean Conservancy, Urban Ocean Marine Debris Prediction"
subtitle: "Trevor Kapuvari, Shreya Bansal, Tianxiao Chen, Stephanie Cheng, Xiaofan Liu"
author: "University of Pennsylvania"
date: "2024-02-22"
output: 
  html_document:
    css: Draft.css
    toc: true
    toc_float: true
    code_folding: "hide"
    theme: flatly
    highlight: tango
    number_sections: yes
mainfont: DejaVu Sans
editor_options: 
  markdown: 
    wrap: 72
---


# 1. Introduction

Pollution is a global problem that takes many forms contaminating every aspect of the environment. Some of this pollution, specifically plastic-waste, ends up in our oceans, harming marine life, contaminating drinking water, and hurting local economies. In response, Ocean Conservancy's Urban Ocean program has been developing projects that mitigate marine pollution, assess waste management, and enable cities to address ocean plastics and resilience. These projects intend to deploy "zero-waste" pilot strategies in cities around the world. With this wide-spanning outreach and diversity in partnerships, an assessment on site selection and resource allocation becomes a repeated step in the process that hinders efficiency each waste-reduction campaign. 

Our objective is to develop a site assessment model that identifies effective zero-waste site locations for national and multinational use. This geospatial risk assessment model serves the purpose of predicting litter accumulation based on globally sourced data with a repeatable framework on an international scale. The results of the model aim to evaluate which sections of a given area have the highest likelihood to produce and contain litter relative to its surroundings. For our case, the Urban Ocean program intends to dedicate "zero-waste" solutions in these areas for the most effective impact for each deployment.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)



library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)
library(RCurl)
library(httr)
library(osmdata)
library(randomForest)
#library(tidygraph)
library(XML)
library(neuralnet)
library(MASS)
library(tidymodels)
library(brms)
library(jsonlite)
library(QuickJSR)
library(hash)
library(fastDummies)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
library(extrafont)
loadfonts()

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
source('https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/function_UO.R')
st_c    <- st_coordinates
st_coid <- st_centroid

# pre-store of osm data need to use 
# a more expandable version of function
# the actual store order is cate label, small cate
stor_df <- data.frame(cato = character(), small =list(), label = character(),stringsAsFactors = FALSE)
add_row <-function(cato,small,label){
  new_row <- list(cato = cato, small = small, label = label)
  stor_df <- bind_rows(stor_df, new_row)
  return(stor_df)
}
stor_df <- add_row('water',list(c('canal','drain','ditch')), 'water')
stor_df <- add_row('amenity',list(c('waste_basket','waste_disposal','waste_transfer_station','recycling')), 'waste')
stor_df <- add_row('amenity',list(c('restaurant','pub','bar')), 'restaurant')
stor_df <- add_row('highway',list('residential'), 'road')
stor_df <- add_row('landuse',list('industrial'), 'industrial')
stor_df <- add_row('landuse',list('residential'), 'residential')
stor_df <- add_row('landuse',list('retail'), 'retail')


plotTheme <- theme(
  plot.title = element_text(size=12, family = "Open Sans", face = "bold"),
  plot.subtitle = element_text(size=8, family = "Open Sans"),
  plot.caption = element_text(size = 6, family = "Open Sans"),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1, family = "Open Sans"),
  axis.text.y = element_text(size = 10, family = "Open Sans"),
  axis.title.y = element_text(size = 10, family = "Open Sans"),
  axis.title.x = element_text(size = 10, family = "Open Sans"),
  plot.background = element_blank(),
  panel.grid.major = element_line(colour = "#489cf4", size = .2),
  axis.ticks = element_blank()
)

mapTheme <- theme(
  plot.title = element_text(size=12, family = "Open Sans", face = "bold"),
  plot.subtitle = element_text(size=7 , family = "Open Sans"),
  plot.caption = element_text(size = 5 , family = "Open Sans"),
  axis.line = element_blank(),
  axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.border = element_blank(),
  panel.grid.major = element_line(colour = 'transparent'),
  panel.grid.minor = element_blank(),
  legend.direction = "vertical", 
  legend.position = "right",
  legend.text = element_text(size = 8, family = "Open Sans"), 
  legend.title = element_text(size = 9, family = "Open Sans", face = "bold"),
  plot.margin = margin(1, 1, 1, 1, 'cm'),
  legend.key.height = unit(1, "cm"), 
  legend.key.width = unit(0.3, "cm")
)
```

# Initial Analysis and Preperation

For our model, we directed our focus on various cities based from the Resilient Cities Network. These cities displayed are Chennai India; Bangkok, Thailand; and Santiago, Chile. The three cities mentioned are apart of a 12 city networking with the goal of proactively enhancing their sanitation infrastructure to combat plastic pollution in their region. The data regarding each region and its attributes was sourced from OpenStreetMap (OSM) to ensure a repeatable framework & quality consistency. 

To develop our model for each respective city (or future use), each area was determined by a 'boundary box' that was created by a custom KML file using Google My Maps (<https://www.google.com/mymaps>). Each boundary's drawing was dictated by political boundaries and practical scope of where data was recorded within the city, further methodology of each individual city is detailed on the interactive dashboard. 
 
## Data Sources and Methodology

In terms of our data & variables, acquiring litter data, our primary environmental health indicator, was acquired via manual download data from Marine Debris Tracker (<https://www.debristracker.org/data/>). The data for each city was carved out by a boundary box determined by initial-research political boundaries. From there, all debris recorded between January 2021 - February 2024 was compiled to a CSV and used as our primary independent variable. Each selected city required its own evaluation in terms of usable debris data over a several year time span. Ideal locations featured several thousand data points across 2-3 years for early & late stage model development. Despite the scalability of the model itself, results and accuracy lay contingent on data quantity and external factors available to each region. 

After the boundary box was created, the data was then projected to a fishnet grid. The fishnet grid was used to count the number of litter points in each cell in a format that can be compared to by each grid cell in the city.  


```{r chennai data, warning = FALSE, message = FALSE, results ='hide'}
#step to load city's data
litter <- read.csv('https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/Data/mdt-dataChennai.csv')

# data filter and projection transformation
litter_p <- litter%>%filter(master_material == 'PLASTIC')%>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant")%>%st_transform('EPSG:32643')

#img <- raster("/Users/mr.smile/Desktop/UPENN/Spring24/CPLN790/data/population_ind_pak_general/population_10_lon_80_general-v1.5.tif")

chen_bdry <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Chennai.kml')
chen_bdry <- st_set_crs(chen_bdry, 4326)%>%st_transform('EPSG:32643')
temp_bd <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Chennai.kml')

temp_bbox <- get_bbox(temp_bd) # get the bounding box (the projection of temp_bd should be epsg4326)
temp_fish <- create_fish(chen_bdry) # get the fishnet of the city (the projection of chen_bdry should be meter degree)
final_net <- countfishnet(temp_fish, litter_p) # create base fishnet with litter (also the one used as final one)
final_net <- pn_gen(stor_df) # add osm point data and knn calculation result into the final dataset
#temp_point <- raster_process(img,temp_bd) # convert the raster file to point one 
#pop_result <- pop_process(temp_point, temp_fish, 32643) # summary the population result
#final_net <- add_pop(pop_result,final_net) # add the pop result into the final dataset
final_net <- moran_gen(final_net,stor_df) # calculate the moran's I result into the dataset
# DONE! 
chen_net <- final_net
```

```{r bangkok data, warning = FALSE, message = FALSE}
litter <- read.csv('https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/Data/mdt-dataBangkok.csv')

litter_b <- litter%>%
  filter(master_material == 'PLASTIC')%>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant")%>%
  st_transform('EPSG:32643')
litter_b <- subset(litter_b, select = -c(event_name, project_name))

bok_bdry <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Bangkok.kml')
bok_bdry <- st_set_crs(bok_bdry, 4326)%>%
  st_transform('EPSG:32643')

bok_bd <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Bangkok.kml')

temp_bbox <- get_bbox(bok_bd) 
temp_fish <- create_fish(bok_bdry)
final_net <- countfishnet(temp_fish, litter_b) 
final_net <- pn_gen(stor_df) 
final_net <- moran_gen(final_net,stor_df) 
bok_net <- final_net
```

```{r santiago data, warning = FALSE, message = FALSE}
litter <- read.csv('https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/Data/mdt-dataSantiago.csv')

# data filter and projection transformation
litter_s <- litter%>%
  filter(master_material == 'PLASTIC')%>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant")%>%
  st_transform('EPSG:32643')
litter_s <- subset(litter_s, select = -c(event_name, project_name))

# load boundary data

san_bdry <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Santiago.kml')
san_bd <- san_bdry
san_bdry <- st_set_crs(san_bdry, 4326)%>%
  st_transform('EPSG:32643')

temp_bbox <- get_bbox(san_bd) 
temp_fish <- create_fish(san_bdry)
final_net <- countfishnet(temp_fish, litter_s) 
final_net <- pn_gen(stor_df) 
final_net <- moran_gen(final_net,stor_df)
san_net <- final_net
```

```{r aggregate data}
chen_net <- chen_net %>%
 # dplyr::select(!c(avg_pop ,sum_pop))%>%
  mutate(city = 'Chennai',
         country = 'India')

san_net <- san_net %>%
  mutate(city = 'Santiago',
         country = 'Chile')

bok_net <- bok_net %>%
  mutate(city = 'Bangkok',
         country = 'Thailand')

tt_net <- rbind(chen_net,san_net,bok_net) %>% mutate(uniqueID = 1:n())

# reference: https://wiki.openstreetmap.org/wiki/Map_features#Entertainment,_Arts_&_Culturee 
leisure <- c('park')
act <- c('maxspeed') 
```

## 2.2 Marine Debris Data, in Summary

The litter data features a wide variety of item categories and general information about each piece. Despite the quality of detail for each recorded sample, this only accounts for litter that has been identified, recorded, and disposed of. This data does not account for litter that was identified but never disposed of, accounted for, or assumed. Regardless, areas where litter was not recorded in a general surrounding could not confidently be assumed to be present or not present. Hence, the purpose of the model is to predict where litter is most likely to accumulate based on the data available. 

Each chart below visualizes a dependent variable reformatted for repeatable use in context that relates to its association with litter accumulation. The examples are, restaurants, roads, retail proximity, and 'significant presence' of restaurants. These are simply the visualized examples and our variables broadly include land-use, proximity to water, waste site facilities, roads, and other indicators of commercial activity. Our variable selection was based on our hypothesis of areas of human activity leading to higher litter risk. The aim of these variables is to act as proxies for litter cases, providing insight on where litter most likely accumulated or ends up. Each variable is then placed on a combined fishnet grid with the litter data, computed using a Chi-Squared test, and evaluated for association between the variables and litter. 

The first map shows the count of restaurants in Chennai, India, similar to the litter data collected, there are few restaurants in any single area, but have tens of each counted throughout that provide insight of the general density/areas of high-activity. 

The second map shows the roads in Chennai, India. Roads indicate urban development and density. All forms of data, whether points, lines, or polygons, are represented onto the fishnet grid to allow for comprehensive data in a uniformed format that allows for ease of comparison and analysis.

The third map shows the 'nearest neighbor' distance of retail in Santiago, Chile. The nearest neighbor analysis is used to find the closest points of the same type. This allows us to find the proximity of another variant to determine commercial/high activity spots in the area. The nearest neighbor factor was repeated for restaurants, land-use, and water. 

Lastly, a significance analysis was conducted on the presence of restaurants in Bangkok, Thailand. The same variables (land-use, water, etc) used for their nearest neighbor factor was also used in their significance aspect. We wanted to examine a 'significance' analysis because of their direct implication with high human activity, excess flow of goods, and interconnection of urban systems. Areas of low distances of a "nearest neighbor" imply restaurants are close to one another geographically, indicate density in resources, and identify an urban core. 

```{r visualize count& continuous}
#visual_count(chen_net,'waste')
#visual_count(chen_net,'water')
visual_count(chen_net,"restaurant")
visual_count(chen_net,'road')
#visual_count(bok_net,'industrial')
#visual_count(chen_net,"residential")
#visual_count(chen_net,'retail')

#visual_cotinuous(chen_net,'waste_nn')
#visual_cotinuous(san_net,'water_nn')
#visual_cotinuous(san_net,"restaurant_nn")
#visual_cotinuous(san_net,'road_nn')
#visual_cotinuous(san_net,'industrial_nn')
#visual_cotinuous(san_net,"residential_nn")
visual_cotinuous(san_net,'retail_nn')

#visual_count(bok_net,'waste_sig')
#visual_count(bok_net,'water_sig')
visual_count(bok_net,"restaurant_sig")
#visual_count(bok_net,'road_sig')
#visual_count(bok_net,'industrial_sig')
#visual_count(bok_net,"residential_sig")
#visual_count(bok_net,'retail_sig')

#visual_cotinuous(bok_net,'waste_sig_dis')
#visual_cotinuous(bok_net,'water_sig_dis')
#visual_cotinuous(bok_net,"restaurant_sig_dis")
#visual_cotinuous(bok_net,'road_sig_dis')
#visual_cotinuous(bok_net,'industrial_sig_dis')
#visual_cotinuous(bok_net,"residential_sig_dis")
#visual_cotinuous(bok_net,'retail_sig_dis')

```



## Principal Component Analysis (idk this part)

The PCA analysis below is used to identify the most important variables in the dataset and their correlation with litter. The reason why we perform PCA is to decrease the dimension of variable. The following result can get explained by the color and direction in the last map. and the last PCA map is the basic for choosing the several variables. Based on the result, the selected variables included 'waste_sig_dis, restaurant_sig_dis, residential_sig_dis, water_sig_dis, residential_nn, industrial_sig_dis, industrial_sig, restaurant_sig, industrial_nn, road_sig_dis, residential_sig, restaurant_sig, restaurant' as the 'shortened model' independent variables.

```{r PCA analysis, message=FALSE, warning=FALSE}
data.pca <- princomp(corr_matrix)
summary(data.pca)
data.pca$loadings[, 1:2]

fviz_eig(data.pca, addlabels = TRUE)

fviz_pca_var(data.pca, col.var = "black")

fviz_cos2(data.pca, choice = "var", axes = 1:2)

fviz_pca_var(data.pca, col.var = "cos2",
            gradient.cols = c("black", "orange", "green"),
            repel = TRUE)
```

# 3. Model Building

## 3.1 Reduced Bias Model

In light of the inherent bias stemmed from litter data, we also are developing a 'Reduced Bias' Model in attempt to compensate for potential  assumptions made by the model in terms of quantity. We take each indicator and coalesce them into a single variable used for our model to compare litter data to the indicators.

```{r model build}
# prepare for model data
df_model <- st_drop_geometry(tt_net)%>%dplyr::select(!cvID)
df_model <- dummy_cols(df_model, select_columns = "city")
df_model <- dummy_cols(df_model, select_columns = "country")
df_model <- df_model %>% dplyr::select(!c(city,country)) %>% mutate(uniqueID = 1:n())

# several model 
temp.rf <- randomForest(count ~ ., data = df_model%>%dplyr::select(!uniqueID), mtry = 10,ntree=70, 
                         importance = TRUE, na.action = na.omit) 

temp.lr <- glm(count ~ ., data = df_model%>%dplyr::select(!uniqueID),family = "poisson", na.action = na.omit) 

temp.lr.qs <- glm(count ~ ., data = df_model%>%dplyr::select(!uniqueID),family = "quasi", na.action = na.omit) 

temp.lr.qp <- glm(count ~ ., data = df_model%>%dplyr::select(!uniqueID),family = "quasipoisson", na.action = na.omit) 

nn_model = neuralnet(count ~ .,data=df_model%>%dplyr::select(!uniqueID),hidden=c(5,2),
linear.output = TRUE
)

temp.hbr <- fit.1<- brm(count~ ., data=df_model%>%dplyr::select(!uniqueID), family=gaussian(),warmup=500, iter=1000, chains=2,cores=2,seed = 1115)
```

## 3.2 Model Results

The following model results of each of the models was added to the dataframe. We test a range of statistical models for a comparative analysis, including random forest, linear model, quasi poisson, hierarchical bayes regression. This section also includes a decision tree model. While this model has a strong visualization of the risk area, its accuracy was not strong.

```{r decision tree model}
set.seed(123)
data_split <- initial_split(df_model, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)

# Create a decision tree model specification
tree_spec <- decision_tree(mode = "regression", tree_depth = 4,engine = 'rpart') 

# Fit the model to the training data
tree_fit <- tree_spec %>%
 fit(count ~ ., data = train_data)

predictions <- tree_fit %>%
 predict(test_data) %>%
 pull(.pred)

metrics <- metric_set(rmse, rsq)
model_performance <- test_data %>%
 mutate(predictions = predictions) %>%
 metrics(truth = count, estimate = predictions)

print(model_performance)

predictions <- tree_fit %>%
 predict(df_model)%>%
 pull(.pred)

df_dt_rst <- df_model%>%
  mutate(Prediction = predictions)%>%
  dplyr::select(uniqueID,count,Prediction)

df_dt_rst <- left_join(final_net%>%dplyr::select(uniqueID),df_dt_rst, by="uniqueID")

#risk_v(df_dt_rst,litter_p,"kmeans")
```

```{r model result}
df_rf_rst <- model_process(df_model,temp.rf)
df_lr_rst <- model_process(df_model,temp.lr)
df_lrqs_rst <- model_process(df_model,temp.lr.qs)
df_lrqp_rst <- model_process(df_model,temp.lr.qp)
df_nn_rst <- model_process(df_model,nn_model)

df_brm_rst <- model_process(df_model,temp.hbr)
estimates_list <- list()
# Iterate over each row by index
for(i in 1:nrow(df_brm_rst)) {
  # Extract the 'Prediction' for the current row
  current_prediction <- df_brm_rst$Prediction[[i]][1]
  estimates_list[[i]] <- current_prediction
}

df_brm_rst$Estimate <- estimates_list
df_brm_rst <- df_brm_rst %>%
  dplyr::select(uniqueID,count,Estimate)%>%
  rename(Prediction = Estimate)

df_brm_rst$Prediction <- as.numeric(unlist(df_brm_rst$Prediction))
#df_brm_r <- model_result(df_model,fit.1) %>%mutate(model = 'BRM')


#df_rf_r <- model_result(df_model,temp.rf) %>%mutate(model = 'RF')
#df_lr_r <- model_result(df_model,temp.lr) %>%mutate(model = 'LR')
#df_lrqs_r <- model_result(df_model,temp.lr.qs)%>%mutate(model = 'LR_quasi')
#df_lrqp_r <- model_result(df_model,temp.lr.qp)%>%mutate(model = 'LR_quasipoisson')
#df_nn_r <- model_result(df_model,nn_model)%>%mutate(model = 'Neural Net')
#df_r_tt <- do.call("rbind", list(df_rf_r, df_lr_r, df_lrqs_r,df_lrqp_r,df_nn_r))

df_rf_rst <- left_join(tt_net%>%dplyr::select(uniqueID,city),df_rf_rst, by="uniqueID")
df_lr_rst <- left_join(tt_net%>%dplyr::select(uniqueID,city),df_lr_rst, by="uniqueID")
df_lrqs_rst <- left_join(tt_net%>%dplyr::select(uniqueID,city),df_lrqs_rst, by="uniqueID")
df_lrqp_rst <- left_join(tt_net%>%dplyr::select(uniqueID,city),df_lrqp_rst, by="uniqueID")
df_brm_rst <- left_join(tt_net%>%dplyr::select(uniqueID,city),df_brm_rst, by="uniqueID")

```

## 3.3 Model Testing Across Different Cities

For our various models, we have tested them on Bangkok as well. The results are displayed below. Each model has a different level of sensitivity.

Below shows the various models for Chennai and their predictions of litter risk across 5 categories of intensity.


```{r model test}


df_rf_c <- df_rf_rst %>% filter(city == 'Chennai')
df_lr_c <- df_lr_rst %>% filter(city == 'Chennai')
df_lrqs_c <- df_lrqs_rst %>% filter(city == 'Chennai')
df_lrqp_c <- df_lrqp_rst %>% filter(city == 'Chennai')
df_brm_c <- df_brm_rst %>% filter(city == 'Chennai')

grid.arrange(
  risk_v(df_rf_c,litter_p,"kmeans",'random forest'),
  risk_v(df_lr_c,litter_p,"kmeans",'linear regression'),
  risk_v(df_lrqs_c,litter_p,"kmeans",'Lr-quasi'),
  risk_v(df_lrqp_c,litter_p,"kmeans",'Lr-quasipossion'), 
  risk_v(df_brm_c,litter_p,"kmeans",'HrB regression'),nrow = 2
)
```

Below shows the various models for Bangkok and their predictions of litter risk across 5 categories of intensity.

```{r bangkok risk_normal}
df_rf_b <- df_rf_rst %>% filter(city == 'Bangkok')
df_lr_b <- df_lr_rst %>% filter(city == 'Bangkok')
df_lrqs_b <- df_lrqs_rst %>% filter(city == 'Bangkok')
df_lrqp_b <- df_lrqp_rst %>% filter(city == 'Bangkok')
df_brm_b <- df_brm_rst %>% filter(city == 'Bangkok')
grid.arrange(
  risk_v(df_rf_b,litter_b,"kmeans",'random forest'),
  risk_v(df_lr_b,litter_b,"kmeans",'linear regression'),
  risk_v(df_lrqs_b,litter_b,"kmeans",'Lr-quasi'),
  risk_v(df_lrqp_b,litter_b,"kmeans",'Lr-quasipossion'),
  risk_v(df_brm_b,litter_b,"kmeans",'HrB regression'),nrow = 2
)
```

The following comparative model visaulizes the strength of the model as it is used acrossed cities.

```{r normal_compare}
grid.arrange(
#risk_v(df_rf_s,litter_s,'kmeans'),
risk_v(df_rf_c,litter_p,'kmeans','random forest'),
risk_v(df_rf_b,litter_b,'kmeans','random forest'),nrow = 1)

grid.arrange(
#risk_v(df_lr_s,litter_s,'kmeans'),
risk_v(df_lr_c,litter_p,'kmeans','linear regression'),
risk_v(df_lr_b,litter_b,'kmeans','linear regression'),nrow = 1)

grid.arrange(
#risk_v(df_lrqs_s,litter_s,'kmeans'),
risk_v(df_lrqs_c,litter_p,'kmeans','Lr-quasi'),
risk_v(df_lrqs_b,litter_b,'kmeans','Lr-quasi'),nrow = 1)

grid.arrange(
#risk_v(df_lrqp_s,litter_s,'kmeans'),
risk_v(df_lrqp_c,litter_p,'kmeans','Lr-quasipossion'),
risk_v(df_lrqp_b,litter_b,'kmeans','Lr-quasipossion'),nrow = 1)
```
