---
title: "Zero-Waste Pilot Site-Selection in Urban Ocean Program Cities"
subtitle: "Analyses for the Marine Litter Assessment Dashboard for Urban Ocean"
author: "Shreya Bansal, Tianxiao Chen, Stephanie Cheng, Trevor Kapuvari, Xiaofan Liu"
date: "2024-05-10"
output: 
  html_document:
    css: Draft.css
    toc: true
    toc_float: true
    code_folding: "hide"
    theme: flatly
    highlight: tango
    number_sections: yes
mainfont: Avenir
editor_options: 
  markdown: 
    wrap: 72
---
<link href="https://fonts.googleapis.com/css2?family=Lexend&display=swap" rel="stylesheet">.

# Introduction

## Abstract

## Background

## Motivation and Use Case

Pollution is a global problem that takes many forms contaminating every aspect of the environment. Some of this pollution, specifically plastic-waste, ends up in our oceans, harming marine life, contaminating drinking water, and hurting local economies. In response, Ocean Conservancy's Urban Ocean program has been developing projects that mitigate marine pollution, assess waste management, and enable cities to address ocean plastics and resilience. These projects intend to deploy "zero-waste" pilot strategies in cities around the world. With this wide-spanning outreach and diversity in partnerships, an assessment on site selection and resource allocation becomes a repeated step in the process that hinders efficiency each waste-reduction campaign. 

Our objective is to develop a site assessment model that identifies effective zero-waste site locations for national and multinational use. This geospatial risk assessment model serves the purpose of predicting litter accumulation based on globally sourced data with a repeatable framework on an international scale. The results of the model aim to evaluate which sections of a given area have the highest likelihood to produce and contain litter relative to its surroundings. For our case, the Urban Ocean program intends to dedicate "zero-waste" solutions in these areas for the most effective impact for each deployment.


```{r setup, include=FALSE, warning = FALSE, message = FALSE, cache = TRUE }
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)
library(RCurl)
library(httr)
library(osmdata)
library(randomForest)
library(XML)
library(neuralnet)
library(MASS)
library(tidymodels)
library(jsonlite)
library(QuickJSR)
library(hash)
library(fastDummies)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
library(extrafont)
library(grid)
library(RColorBrewer)
library(caTools)
library(dials) 
library(ranger)
library(xgboost)
library(Metrics)
library(caret)

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
source('https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/function_UO.R')
st_c    <- st_coordinates
st_coid <- st_centroid

risk_palette <- c("#FBEDC6", "#F8D7AD", "#F3C0A6", "#DE8595", "#BF6F8A")

cities <- c("Bangkok", "Can_Tho", "Chennai", "Melaka", "Mumbai", "Panama_City", 
            "Pune", "Salvador", "Santa_Fe", "Santiago", "Semarang", "Surat")

bd <- c("Bangkok_bd", "Can_Tho_bd", "Chennai_bd", "Melaka_bd", "Mumbai_bd", "Panama_City_bd", "Pune_bd", "Salvador_bd", "Santa_Fe_bd", "Santiago_bd", "Semarang_bd", "Surat_bd")

bdM <- c("Bangkok_bdM", "Can_Tho_bdM", "Chennai_bdM", "Melaka_bdM", "Mumbai_bdM", "Panama_City_bdM","Pune_bdM", "Salvador_bdM", "Santa_Fe_bdM", "Santiago_bdM", "Semarang_bdM", "Surat_bdM")


# pre-store of osm data need to use 
# a more expandable version of function
# the actual store order is cate label, small cate
stor_df <- data.frame(cato = character(), small =list(), label = character(),stringsAsFactors = FALSE)
add_row <-function(cato,small,label){
  new_row <- list(cato = cato, small = small, label = label)
  stor_df <- bind_rows(stor_df, new_row)
  return(stor_df)
}
stor_df <- add_row('water',list(c('canal','drain','ditch')), 'water')
stor_df <- add_row('amenity',list(c('waste_basket','waste_disposal','waste_transfer_station','recycling')), 'waste')
stor_df <- add_row('amenity',list(c('restaurant','pub','bar')), 'restaurant')
stor_df <- add_row('highway',list('residential'), 'road')
stor_df <- add_row('landuse',list('industrial'), 'industrial')
stor_df <- add_row('landuse',list('residential'), 'residential')
stor_df <- add_row('landuse',list('retail'), 'retail')

plotTheme <- function(base_size = 10, title_size = 14) {
  theme(
    text = element_text(family = "Avenir", color = "black"),
    plot.title = element_text(size = title_size, family = "Avenir", colour = "black"), 
    plot.subtitle = element_text(face = "italic", family = "Avenir"),
    plot.caption = element_text(hjust = 0, family = "Avenir"),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    #panel.border = element_rect(colour = "black", fill = NA, size = 2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size = 10, family = "Avenir"),
    axis.title = element_text(size = 10, family = "Avenir"),
    axis.text = element_text(size = 8, family = "Avenir"),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic", family = "Avenir"),
    legend.text = element_text(colour = "black", face = "italic", family = "Avenir"),
    strip.text.x = element_text(size = 12, family = "Avenir")
  )
}

mapTheme <- function(base_size = 10, title_size = 14) {
  theme(
    text = element_text(family = "Avenir", color = "black"),
    plot.title = element_text(size = title_size, colour = "black", family = "Avenir"),
    plot.subtitle = element_text(face = "italic", family = "Avenir"),
    plot.caption = element_text(hjust = 0, family = "Avenir"),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    #panel.border = element_rect(colour = "black", fill = NA, size = 2),
    strip.text.x = element_text(size = 12, family = "Avenir")
  )
}

```

# Exploratory Data Analysis

## Understanding the Data

### Marine Debris Tracker


### OpenStreetMap Data


### Other Data

For our model, we directed our focus on data modeling from the Urban Ocean cities. To develop our model for each respective city (or future use), each area was determined by a 'boundary box' that was created by a custom KML file using Google My Maps (<https://www.google.com/mymaps>) or sourced online. Each boundary's drawing was dictated by political boundaries and practical scope of where data was recorded within the city, further methodology of each individual city is detailed on the interactive dashboard. 
 

In terms of our data & variables, acquiring litter data, our primary environmental health indicator, was acquired via manual download data from Marine Debris Tracker (<https://www.debristracker.org/data/>). The data for each city was carved out by a boundary box determined by initial-research political boundaries. From there, all debris recorded between January 2021 - February 2024 was compiled to a CSV and used as our primary independent variable. Each selected city required its own evaluation in terms of usable debris data over a several year time span. Ideal locations featured several thousand data points across 2-3 years for early & late stage model development. Despite the scalability of the model itself, results and accuracy lay contingent on data quantity and external factors available to each region. 

After the boundary box was created, the data was then projected to a fishnet grid. The fishnet grid was used to count the number of litter points in each cell in a format that can be compared to by each grid cell in the city.  

## Feature Engineering
```{r chennai data, warning = FALSE, message = FALSE, results ='hide', cache = TRUE}
#step to load city's data
litter <- read.csv('https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/Data/mdt-dataChennai.csv')

# data filter and projection transformation
litter_p <- litter%>%filter(master_material == 'PLASTIC')%>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant")%>%st_transform('EPSG:32643')

#img <- raster("/Users/mr.smile/Desktop/UPENN/Spring24/CPLN790/data/population_ind_pak_general/population_10_lon_80_general-v1.5.tif")

chen_bdry <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Chennai.kml')
chen_bdry <- st_set_crs(chen_bdry, 4326)%>%st_transform('EPSG:32643')
temp_bd <- st_read('https://github.com/TrevorKap/MUSA810-Marine-Pollution/raw/main/Data/Chennai.kml')

temp_bbox <- get_bbox(temp_bd) # get the bounding box (the projection of temp_bd should be epsg4326)
temp_fish <- create_fish(chen_bdry) # get the fishnet of the city (the projection of chen_bdry should be meter degree)
final_net <- countfishnet(temp_fish, litter_p) # create base fishnet with litter (also the one used as final one)
final_net <- pn_gen(stor_df) # add osm point data and knn calculation result into the final dataset
#temp_point <- raster_process(img,temp_bd) # convert the raster file to point one 
#pop_result <- pop_process(temp_point, temp_fish, 32643) # summary the population result
#final_net <- add_pop(pop_result,final_net) # add the pop result into the final dataset
final_net <- moran_gen(final_net,stor_df) # calculate the moran's I result into the dataset
# DONE! 
chen_net <- final_net
```

```{r data loading, warning = FALSE, message = FALSE, cache = TRUE }
city_data_list <- lapply(cities, load_city_data)
names(city_data_list) <- cities

bd_data_list <- lapply(cities, load_city_kml)
names(bd_data_list) <- bd

bd_data_meter_list <- lapply(cities, load_city_kml_meter)
names(bd_data_meter_list) <- bdM

for (i in seq_along(city_data_list)) {assign(cities[i], city_data_list[[i]])}

for (i in seq_along(bd_data_list)) {assign(bd[i], bd_data_list[[i]])}

for (i in seq_along(bd_data_meter_list)) {assign(bdM[i], bd_data_meter_list[[i]])}
```

```{r loading other nets, warning = FALSE, message = FALSE, cache = TRUE }

base_path <- "https://raw.githubusercontent.com/TrevorKap/MUSA810-Marine-Pollution/main/Data/stored_city/"
city_names <- c("santa_fe", "semarang", "panama", "can_tho", "melaka", "salvador",
                "surat", "santiago", "bangkok", "chennai", "mumbai", "pune")
city_data <- list()

for (city in city_names) {
  file_path <- paste0(base_path, city, "_net.geojson")
  net_sf <- st_read(file_path)
  net_sf <- st_set_crs(net_sf, 32643)
  city_data[[city]] <- net_sf
}
list2env(setNames(city_data, paste0("net_", city_names)), envir = .GlobalEnv)

```

```{r, warning = FALSE, message = FALSE, cache = TRUE }
cnt_wt <- function(litter,net){
  temp <- st_join(litter,net,join = st_within)
  temp <- temp %>% filter(!is.na(uniqueID))
  temp_sum <- temp %>%
    group_by(uniqueID)%>%
    summarise(count_un = n_distinct(username))
  net <- left_join(net,st_drop_geometry(temp_sum),by = 'uniqueID')
  net <- net %>%
    mutate(count_un = replace_na(count_un, 0),
         count = count/count_un,
         count = replace_na(count, 0))
  return(net)
}

net_mumbai <- cnt_wt(Mumbai,net_mumbai)
net_chennai <- cnt_wt(Chennai,net_chennai)
net_bangkok <- cnt_wt(Bangkok,net_bangkok)
net_can_tho <- cnt_wt(Can_Tho,net_can_tho)
net_melaka <- cnt_wt(Melaka,net_melaka)
net_panama <- cnt_wt(Panama_City,net_panama)
net_pune <- cnt_wt(Pune,net_pune)
net_salvador <- cnt_wt(Salvador,net_salvador)
net_santa_fe <- cnt_wt(Santa_Fe,net_santa_fe)
net_santiago <- cnt_wt(Santiago,net_santiago)
net_semarang <- cnt_wt(Semarang,net_semarang)
net_surat <- cnt_wt(Surat,net_surat)
```


```{r z-score, warning = FALSE, message = FALSE, cache = TRUE }
z_score_normalize <- function(x) {
  (x - mean(x)) / sd(x)
}

normal_city <- function(data){
  data <- data%>% 
    mutate(across(where(is.numeric) & -'count', z_score_normalize),
           across(where(is.numeric), ~replace(., is.nan(.), 0)))
  return(data)
}

net_salvador <- normal_city(net_salvador)
net_santa_fe <- normal_city(net_santa_fe)
net_santiago <- normal_city(net_santiago)
net_semarang <- normal_city(net_semarang)
net_surat <- normal_city(net_surat)
net_bangkok <- normal_city(net_bangkok)
net_can_tho <- normal_city(net_can_tho)
net_chennai <- normal_city(net_chennai)
net_melaka <- normal_city(net_melaka)
net_mumbai <- normal_city(net_mumbai)
net_panama <- normal_city(net_panama)
net_pune <- normal_city(net_pune)
```

```{r aggregate data, warning = FALSE, message = FALSE, cache = TRUE }
net_bangkok <- net_bangkok %>% mutate(city = 'Bangkok',country = 'Thailand')
net_can_tho <- net_can_tho %>% mutate(city = 'Can_Tho',country = 'Vietnam')
net_chennai <- net_chennai %>% mutate(city = 'Chennai',country = 'India')
net_melaka <- net_melaka %>% mutate(city = 'Melaka',country = 'Malaysia')
net_mumbai <- net_mumbai %>% mutate(city = 'Mumbai',country = 'India')
net_panama <- net_panama %>% mutate(city = 'Panama_City',country = 'Panama')
net_pune <- net_pune %>% mutate(city = 'Pune',country = 'India')
net_salvador <- net_salvador %>% mutate(city = 'Salvador',country = 'Brazil')
net_santa_fe <- net_santa_fe %>% mutate(city = 'Santa_Fe',country = 'Argentina')
net_santiago <- net_santiago %>% mutate(city = 'Santiago',country = 'Chile')
net_semarang <- net_semarang %>% mutate(city = 'Semarang',country = 'Indonesia')
net_surat <- net_surat %>% mutate(city = 'Surat',country = 'India')

net_total <- rbind(net_bangkok,net_can_tho,net_chennai,net_melaka,net_mumbai,net_panama,net_pune,net_salvador,net_santa_fe,net_santiago,net_semarang,net_surat) %>% mutate(uniqueID = 1:n())

net_total_li <- net_total %>% dplyr::filter(count != 0)
net_total_li <- net_total_li %>%
  mutate(count = (count - mean(count)) / sd(count))

```



## 

The litter data features a wide variety of item categories and general information about each piece. Despite the quality of detail for each recorded sample, this only accounts for litter that has been identified, recorded, and disposed of. This data does not account for litter that was identified but never disposed of, accounted for, or assumed. Regardless, areas where litter was not recorded in a general surrounding could not confidently be assumed to be present or not present. Hence, the purpose of the model is to predict where litter is most likely to accumulate based on the data available. 

Each chart below visualizes a dependent variable reformatted for repeatable use in context that relates to its association with litter accumulation. The examples are, restaurants, roads, retail proximity, and 'significant presence' of restaurants. These are simply the visualized examples and our variables broadly include land-use, proximity to water, waste site facilities, roads, and other indicators of commercial activity. Our variable selection was based on our hypothesis of areas of human activity leading to higher litter risk. The aim of these variables is to act as proxies for litter cases, providing insight on where litter most likely accumulated or ends up. Each variable is then placed on a combined fishnet grid with the litter data, computed using a Chi-Squared test, and evaluated for association between the variables and litter. 

The first map shows the count of restaurants in Chennai, India, similar to the litter data collected, there are few restaurants in any single area, but have tens of each counted throughout that provide insight of the general density/areas of high-activity. 

The second map shows the roads in Chennai, India. Roads indicate urban development and density. All forms of data, whether points, lines, or polygons, are represented onto the fishnet grid to allow for comprehensive data in a uniformed format that allows for ease of comparison and analysis.

The third map shows the 'nearest neighbor' distance of retail in Santiago, Chile. The nearest neighbor analysis is used to find the closest points of the same type. This allows us to find the proximity of another variant to determine commercial/high activity spots in the area. The nearest neighbor factor was repeated for restaurants, land-use, and water. 

Lastly, a significance analysis was conducted on the presence of restaurants in Bangkok, Thailand. The same variables (land-use, water, etc) used for their nearest neighbor factor was also used in their significance aspect. We wanted to examine a 'significance' analysis because of their direct implication with high human activity, excess flow of goods, and interconnection of urban systems. Areas of low distances of a "nearest neighbor" imply restaurants are close to one another geographically, indicate density in resources, and identify an urban core. 

```{r waste visualization grid, warning = FALSE, message = FALSE, cache = TRUE}
map_waste <- visual_count(net_chennai, "waste")
map_waste_nn <- visual_count(net_chennai,'waste_nn')

grid.arrange(map_waste, map_waste_nn, ncol = 2)

```



```{r visualize count, warning = FALSE, message = FALSE, cache = TRUE }

map_water <- visual_count(net_chennai,"water")
map_restaurant <- visual_count(net_chennai,"restaurant")
map_road <- visual_count(net_chennai,'road')
map_industrial <- visual_count(net_chennai,'industrial')
map_residential <- visual_count(net_chennai,"residential")
map_retail <- visual_count(net_chennai,'retail')

grid.arrange(map_water, map_restaurant, map_road, map_industrial,
             map_residential, map_retail,
             ncol = 2)


```

```{r visualize nearest neighbour, warning = FALSE, message = FALSE, cache = TRUE }

map_water_nn <- visual_count(net_chennai,'water_nn')
map_restaurant_nn <- visual_count(net_chennai,"restaurant_nn")
map_road_nn <- visual_count(net_chennai,'road_nn')
map_industrial_nn <- visual_count(net_chennai,'industrial_nn')
map_residential_nn <- visual_count(net_chennai,"residential_nn")
map_retail_nn <- visual_count(net_chennai,'retail_nn')

grid.arrange(map_water_nn, map_restaurant_nn, map_road_nn,
             map_industrial_nn, map_residential_nn, map_retail_nn,
             ncol = 2)
```


## Principal Component Analysis

The PCA analysis below is used to identify the most important variables in the dataset and their correlation with litter. The reason why we perform PCA is to decrease the dimension of variable. The following result can get explained by the color and direction in the last map. and the last PCA map is the basic for choosing the several variables. Based on the result, the selected variables included 'waste_sig_dis, restaurant_sig_dis, residential_sig_dis, water_sig_dis, residential_nn, industrial_sig_dis, industrial_sig, restaurant_sig, industrial_nn, road_sig_dis, residential_sig, restaurant_sig, restaurant' as the 'shortened model' independent variables.

```{r PCA analysis, warning = FALSE, message = FALSE, cache = TRUE }
corr <- st_drop_geometry(net_total) %>% dplyr::select(!c(uniqueID,cvID,city,country))
corr_nor<- scale(corr)
corr_matrix <- cor(corr_nor)
#ggcorrplot(corr_matrix)

data.pca <- princomp(corr_matrix)
summary(data.pca)
data.pca$loadings[, 1:2]
fviz_eig(data.pca, addlabels = TRUE) + ggtitle("Eigenvalues from Principal Component Analysis") + plotTheme()
fviz_pca_var(data.pca, col.var = "black")+ plotTheme()
fviz_cos2(data.pca, choice = "var", axes = 1:2)+ plotTheme()
n_colors <- 100 
mako_gradient <- viridis(n_colors, option = "mako")
fviz_pca_var(data.pca, col.var = "cos2",
             gradient.cols = mako_gradient,
             repel = TRUE) + plotTheme()

```


# Model Building

## Random Forest Model 1

The following random forest model utilizes the TidyModels package.
```{r rf tidymodels, warning = FALSE, message = FALSE, cache = TRUE }
set.seed(123)
net_tt_nor <- st_drop_geometry(net_total_li) %>%
  dplyr::select(-c(uniqueID,cvID,city,country))
net_tt_nor$city <- net_total_li$city
net_tt_nor$uniqueID <- net_total_li$uniqueID
net_tt_temp <- net_tt_nor%>%dplyr::select(!c(uniqueID,city))

net_tt_split <- initial_split(net_tt_temp, prop = 0.75)
train_data <- training(net_tt_split)
test_data <- testing(net_tt_split)

rf_model <- rand_forest(
  mode = "regression",
  trees = tune(),         
  min_n = tune(),
  mtry = tune(),
) %>% set_engine("ranger")

rf_recipe <- recipe(count ~ ., data = train_data)

cv_folds <- vfold_cv(train_data, v = 10)

rf_grid <- rf_grid <- grid_regular(
  trees(range = c(500, 2000)),   
  min_n(range = c(9, 15)),       
  mtry(range = c(6, 30)),        
  levels = c(4, 3, 5) 
)

rf_workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_results <- tune_grid(
  rf_workflow,
  resamples = cv_folds,
  grid = rf_grid
)

show_best(rf_results, metric = "rmse")
```

```{r final_rf, warning = FALSE, message = FALSE, cache = TRUE }
final_rf <- finalize_workflow(
  rf_workflow,
  select_best(rf_results, metric = "rmse")
)

final_rf <- last_fit(final_rf, split = net_tt_split )

#results <- collect_metrics(final_rf)
#print(results)

temp_ts <- predict(final_rf$.workflow[[1]], test_data) %>%
  rename(Prediction = .pred)
ts_bind <- cbind(test_data,temp_ts)

#best_results <- select_best(rf_results, metric = "rmse")
temp <- predict(final_rf$.workflow[[1]], net_total) %>%
  rename(Prediction = .pred)
temp_cat <- risk_level(temp,'kmeans')
df_rf_rst <- cbind(net_total,temp_cat)
```

## Linear Model
```{r linear model, warning = FALSE, message = FALSE, cache = TRUE }
set.seed(223)
train_control <- trainControl(method = "cv", number = 60)

# Train the model
model <- train(count ~ ., data = train_data, method = "lm",family = "quasi",trControl = train_control)

test_temp <- predict(model,net_total)
test_cbind <- cbind(net_total,test_temp)%>%
  rename(Prediction = test_temp)

temp_risk_cat <- st_drop_geometry(risk_level(test_cbind,'kmeans')) %>%dplyr::select(Risk_Category)
test_cbind <- cbind(test_cbind,temp_risk_cat)

city_viz('Chennai',test_cbind,'Random Forest Model')
```

## Multi-Vision Model Evaluation
```{r multi-vision model eval, warning = FALSE, message = FALSE, cache = TRUE }
# rf model result -- ts_bind

rf_test <- ts_bind %>% dplyr::select(count,Prediction)

lr_rst <- predict(model,test_data)
lr_test <- cbind(test_data,lr_rst)%>% rename(Prediction = lr_rst) %>% dplyr::select(count,Prediction)

rf_test$Residuals <- rf_test$count - rf_test$Prediction
lr_test$Residuals <- lr_test$count - lr_test$Prediction

ggplot(rf_test, aes(x = count, y = Prediction)) +
  geom_point() +  # Add points
  geom_segment(aes(xend = count, yend = Prediction, x = count, y = count), 
               linetype = "dotted", color = "red") +  # Residual lines
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "blue") +
  labs(x = "Actual Value", y = "Predicted Value", title = "RandomForest Model Residuals") +
  plotTheme()

ggplot(lr_test, aes(x = count, y = Prediction)) +
  geom_point() +  # Add points
  geom_segment(aes(xend = count, yend = Prediction, x = count, y = count), 
               linetype = "dotted", color = "red") +  # Residual lines
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "blue") +
  labs(x = "Actual Value", y = "Predicted Value", title = "Linear Regression Model Residuals") +
  plotTheme()

ggplot(lr_test, aes(x = Residuals)) + 
  geom_histogram(bins = 30, fill = "grey") + plotTheme()

ggplot(rf_test, aes(x = Residuals)) + 
  geom_histogram(bins = 30, fill = "grey") + plotTheme()
```

```{r mixed rst, warning = FALSE, message = FALSE, cache = TRUE }
mixed_rst <- cbind(lr_test%>%dplyr::select(-Residuals),rf_test$Prediction)%>%
  rename(lr_pred = Prediction,
         rf_pred = 'rf_test$Prediction') %>%
  mutate(Prediction = 0.5*rf_pred + 0.5*lr_pred,
         Residuals = count - Prediction)

ggplot(mixed_rst, aes(x = Residuals)) + 
  geom_histogram(bins = 30, fill = "grey")+ 
  ggtitle("Histogram of Residuals") +  # Add a title
  xlab("Values") +  # Label for the x-axis
  ylab("Frequency") +  # Label for the y-axis
  plotTheme()

grid.arrange(
ggplot(mixed_rst, aes(x = Residuals)) + 
  geom_histogram(bins = 30, fill = "grey") + plotTheme() ,
ggplot(lr_test, aes(x = Residuals)) + 
  geom_histogram(bins = 30, fill = "grey") + plotTheme() ,
ggplot(rf_test, aes(x = Residuals)) + 
  geom_histogram(bins = 30, fill = "grey") + plotTheme() ,nrow = 3)

ggplot(mixed_rst, aes(x = count, y = Prediction)) +
  geom_point() +  # Add points
  geom_segment(aes(xend = count, yend = Prediction, x = count, y = count), 
               linetype = "dotted", color = "red") +  # Residual lines
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "blue") +
  labs(x = "Actual Value", y = "Predicted Value", title = 'Mixed Model Residuals') +
  plotTheme()

ggplot(mixed_rst, aes(sample = Residuals)) +
  stat_qq() +
  stat_qq_line(colour = "red") +
  ggtitle("Normal Q-Q Plot") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles") +
  plotTheme()
```

## Multi-Vision Model Build
```{r multi-vision model build, warning = FALSE, message = FALSE, cache = TRUE }
test_cb_rst <- st_drop_geometry(test_cbind) %>%
  dplyr::select(Prediction,Risk_Category)%>%
  rename(Pre_lr = Prediction,
         Risk_lr = Risk_Category)
rst_total <- cbind(df_rf_rst,test_cb_rst)%>%
  rename(Pre_rf = Prediction,
         Risk_rf = Risk_Category)
rst_total <- rst_total %>%
  mutate(Prediction = 0.4*Pre_rf + 0.6*Pre_lr)
rst_total <- risk_level(rst_total,'kmeans')

grid.arrange(
city_viz('Chennai',test_cbind,'Linear Regression Model'),
city_viz('Chennai',df_rf_rst,'Random Forest Model'),
city_viz('Chennai',rst_total,'Mixed Model'),nrow = 1)

```
# Model Results

```{r all risk maps mixed model, warning = FALSE, message = FALSE, cache = TRUE }
risk_v_new <-function(model_data,litter_data,model,city){
  model_data$Risk_Category <- as.factor(model_data$Risk_Category)
  ggplot() +
    geom_sf(data = model_data, aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = litter_data, size = .3, colour = "red") +
    scale_fill_manual(values = risk_palette, guide = "none") +
    labs(title=model) +
    mapTheme(title_size = 8)
}
city_viz_new <- function(cities,data,model){
  temp <- data %>% filter(city == cities)
  risk_v_new(temp,get(cities),model,cities)
}

risk_Chennai <- city_viz_new('Chennai', rst_total, 'Chennai')
risk_Bangkok <- city_viz_new('Bangkok', rst_total, 'Bangkok')
risk_Can_Tho <- city_viz_new('Can_Tho', rst_total, 'Can Tho')
risk_Melaka <- city_viz_new('Melaka', rst_total, 'Melaka')
risk_Mumbai <- city_viz_new('Mumbai', rst_total, 'Mumbai')
risk_Panama_City <- city_viz_new('Panama_City', rst_total, 'Panama City')
risk_Pune <- city_viz_new('Pune', rst_total, 'Pune')
risk_Salvador <- city_viz_new('Salvador', rst_total, 'Salvador')
risk_Santa_Fe <- city_viz_new('Santa_Fe', rst_total, 'Santa Fe')
risk_Santiago <- city_viz_new('Santiago', rst_total, 'Santiago')
risk_Semarang <- city_viz_new('Semarang', rst_total, 'Semarang')
risk_Surat <- city_viz_new('Surat', rst_total, 'Surat')

      
# Grid arrange the plots
grid.arrange(risk_Chennai, risk_Bangkok, risk_Can_Tho, risk_Melaka, risk_Mumbai,
             risk_Panama_City, risk_Pune, risk_Salvador, risk_Santa_Fe,
             risk_Santiago, risk_Semarang, risk_Surat, nrow = 4)
```


# Model and Error Analysis

# Web-Based Dashboard

# Conclusion

# Sources

Data Sources
* 

Image Sources
* 


